
Äá»“ Ã¡n cuá»‘i ká»³ mÃ´n Python cho khoa há»c dá»¯ liá»‡u



````markdown
# ğŸ¥ Dá»± Ä‘oÃ¡n Chi PhÃ­ Y Táº¿ (Medical Cost Prediction)

Dá»± Ã¡n Machine Learning End-to-End nháº±m dá»± Ä‘oÃ¡n chi phÃ­ y táº¿ háº±ng nÄƒm (`annual_medical_cost`) dá»±a trÃªn há»“ sÆ¡ nhÃ¢n kháº©u há»c, sá»©c khá»e vÃ  báº£o hiá»ƒm cá»§a bá»‡nh nhÃ¢n. Dá»± Ã¡n Ä‘Æ°á»£c xÃ¢y dá»±ng theo hÆ°á»›ng Ä‘á»‘i tÆ°á»£ng (OOP) vá»›i cÃ¡c module tÃ¡i sá»­ dá»¥ng cao.

## ğŸ¯ Bá»‘i cáº£nh & Má»¥c tiÃªu
Chi phÃ­ chÄƒm sÃ³c sá»©c khá»e Ä‘ang lÃ  gÃ¡nh náº·ng lá»›n Ä‘á»‘i vá»›i nhiá»u cÃ¡ nhÃ¢n vÃ  tá»• chá»©c báº£o hiá»ƒm. Dá»± Ã¡n nÃ y Ä‘Æ°á»£c xÃ¢y dá»±ng nháº±m giáº£i quyáº¿t bÃ i toÃ¡n: **"Liá»‡u cÃ³ thá»ƒ dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c chi phÃ­ y táº¿ háº±ng nÄƒm dá»±a trÃªn há»“ sÆ¡ cÃ¡ nhÃ¢n?"**

Dá»¯ liá»‡u bao gá»“m 100.000 báº£n ghi vá»›i cÃ¡c nhÃ³m thÃ´ng tin:
* **Demographics:** Tuá»•i, giá»›i tÃ­nh, vÃ¹ng miá»n.
* **Lifestyle:** Chá»‰ sá»‘ BMI, hÃºt thuá»‘c, táº­p thá»ƒ dá»¥c.
* **Medical History:** Tiá»n sá»­ bá»‡nh lÃ½ (tiá»ƒu Ä‘Æ°á»ng, cao huyáº¿t Ã¡p...).
* **Insurance:** Loáº¡i gÃ³i báº£o hiá»ƒm, háº¡n má»©c.

**Má»¥c tiÃªu chÃ­nh:** XÃ¢y dá»±ng mÃ´ hÃ¬nh há»“i quy (Regression) Ä‘á»ƒ dá»± Ä‘oÃ¡n `annual_medical_cost`, giÃºp cÃ¡c cÃ´ng ty báº£o hiá»ƒm Ä‘Ã¡nh giÃ¡ rá»§i ro vÃ  cÃ¡ nhÃ¢n hÃ³a gÃ³i dá»‹ch vá»¥.

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![Library](https://img.shields.io/badge/Library-Scikit--Learn%20%7C%20Pandas%20%7C%20Seaborn-green)

## ğŸŒŸ TÃ­nh nÄƒng ná»•i báº­t

* **Pipeline Tiá»n xá»­ lÃ½ dá»¯ liá»‡u máº¡nh máº½ (`preprocess.py`):**
    * Tá»± Ä‘á»™ng xá»­ lÃ½ giÃ¡ trá»‹ thiáº¿u (Mean, Median, Forward-fill).
    * PhÃ¡t hiá»‡n vÃ  xá»­ lÃ½ ngoáº¡i lai (Outliers) báº±ng IQR hoáº·c Isolation Forest.
    * Feature Engineering: TÃ¡ch ngÃ y thÃ¡ng, One-Hot Encoding, Ordinal Encoding.
    * Chuáº©n hÃ³a dá»¯ liá»‡u (Scaling) Ä‘á»ƒ chá»‘ng rÃ² rá»‰ dá»¯ liá»‡u (Data Leakage).
* **Huáº¥n luyá»‡n mÃ´ hÃ¬nh tá»± Ä‘á»™ng (`model_trainer.py`):**
    * Há»— trá»£ cháº¡y Ä‘ua (Race) giá»¯a nhiá»u mÃ´ hÃ¬nh: Random Forest, XGBoost, Linear Regression, SVM...
    * Tá»± Ä‘á»™ng tá»‘i Æ°u tham sá»‘ (Hyperparameter Tuning) báº±ng Random Search.
    * LÆ°u trá»¯ artifact trá»n gÃ³i (Model + Scaler + Metrics).
* **Trá»±c quan hÃ³a dá»¯ liá»‡u (`visualize.py`):**
    * Há»‡ thá»‘ng váº½ biá»ƒu Ä‘á»“ chuáº©n hÃ³a, dá»… dÃ ng so sÃ¡nh hiá»‡u suáº¥t mÃ´ hÃ¬nh.
    * Há»— trá»£ váº½ Dashboard so sÃ¡nh Ä‘a chá»‰ sá»‘ (RMSE, MAE, R2).

## ğŸ“‚ Cáº¥u trÃºc dá»± Ã¡n

```text
â”œâ”€â”€ data/                       # Chá»©a file dá»¯ liá»‡u gá»‘c (csv, xlsx)
â”œâ”€â”€ results/                    # Káº¿t quáº£ Ä‘áº§u ra (Logs, Models, Charts)
â”‚   â”œâ”€â”€ best_model.pkl          # Model tá»‘t nháº¥t Ä‘Ã£ huáº¥n luyá»‡n
â”‚   â”œâ”€â”€ training.log            # Log quÃ¡ trÃ¬nh cháº¡y
â”‚   â””â”€â”€ model_comparison.csv    # Báº£ng so sÃ¡nh cÃ¡c model
â”‚   â”œâ”€â”€ cÃ¡c plot so sÃ¡nh cÃ¡c model
â”œâ”€â”€ preprocess.py               # Module lÃ m sáº¡ch vÃ  biáº¿n Ä‘á»•i dá»¯ liá»‡u
â”œâ”€â”€ visualize.py                # Module trá»±c quan hÃ³a
â”œâ”€â”€ model_trainer.py            # Module quáº£n lÃ½ huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡
â”œâ”€â”€ EDA.ipynb                   # Notebook Ä‘á»ƒ cháº¡y pháº§n EDA cá»§a dá»± Ã¡n
â”œâ”€â”€ FE_MODELING.ipynb           # Notebook Ä‘á»ƒ cháº¡y feature engineering vÃ  modeling
â”œâ”€â”€ requirements.txt            # CÃ¡c thÆ° viá»‡n cáº§n thiáº¿t
â””â”€â”€ README.md                   # HÆ°á»›ng dáº«n sá»­ dá»¥ng
````

## ğŸ› ï¸ CÃ i Ä‘áº·t

### 1\. YÃªu cáº§u há»‡ thá»‘ng

  * Python 3.8 trá»Ÿ lÃªn.
  * CÃ¡c thÆ° viá»‡n: pandas, numpy, scikit-learn, matplotlib, seaborn, joblib, xgboost, lightgbm.

### 2\. CÃ i Ä‘áº·t thÆ° viá»‡n

Cháº¡y lá»‡nh sau Ä‘á»ƒ cÃ i Ä‘áº·t cÃ¡c gÃ³i cáº§n thiáº¿t:

```bash
pip install -r requirements.txt
```



-----

## ğŸš€ HÆ°á»›ng dáº«n sá»­ dá»¥ng

### BÆ°á»›c 1: Chuáº©n bá»‹ dá»¯ liá»‡u vÃ  Tiá»n xá»­ lÃ½

Sá»­ dá»¥ng `DataManager` vÃ  cÃ¡c bá»™ xá»­ lÃ½ trong `preprocess.py`.

```python
from preprocess import DataManager, Imputer, Scaler, FeatureEngineer, OutlierHandler

# 1. Load dá»¯ liá»‡u
manager = DataManager('data/medical_cost.csv')

# 2. Äá»‹nh nghÄ©a Pipeline
steps = [
    Imputer(strategy='mean', columns=['bmi', 'income']),
    FeatureEngineer(one_hot_cols=['region', 'smoker'], ordinal_cols={'education': ['No HS', 'HS', 'Bachelor']}),
    OutlierHandler(method='isolation_forest', action='remove'),
    Scaler(method='standard')
]

# 3. Ãp dá»¥ng
for step in steps:
    manager.apply(step)

df_clean = manager.get_data()
```

### BÆ°á»›c 2: Huáº¥n luyá»‡n vÃ  So sÃ¡nh mÃ´ hÃ¬nh

Sá»­ dá»¥ng `ModelTrainer` Ä‘á»ƒ tá»± Ä‘á»™ng tÃ¬m mÃ´ hÃ¬nh tá»‘t nháº¥t.

```python
from model_trainer import ModelTrainer
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

# 1. Khá»Ÿi táº¡o
trainer = ModelTrainer(task_type='regression')
trainer.load_data(df_clean.drop('annual_medical_cost', axis=1), df_clean['annual_medical_cost'])
trainer.split_data()

# 2. Cáº¥u hÃ¬nh cÃ¡c model cáº§n Ä‘ua
models_config = {
    'RandomForest': (RandomForestRegressor(), {'n_estimators': [50, 100]}),
    'GradientBoosting': (GradientBoostingRegressor(), {'learning_rate': [0.01, 0.1]})
}

# 3. Cháº¡y tá»± Ä‘á»™ng
trainer.auto_train(models_config, output_dir='results')
```

### BÆ°á»›c 3: ÄÃ¡nh giÃ¡ vÃ  Trá»±c quan hÃ³a

Sá»­ dá»¥ng `DataVisualizer` Ä‘á»ƒ xem káº¿t quáº£.

```python
import pandas as pd
from visualize import DataVisualizer

# 1. Äá»c káº¿t quáº£ so sÃ¡nh
df_results = pd.read_csv('results/model_comparison.csv')

# 2. Váº½ biá»ƒu Ä‘á»“ so sÃ¡nh RMSE
viz = DataVisualizer(df_results)
viz.plot_bar(x='score', y='model', title='So sÃ¡nh RMSE (Tháº¥p hÆ¡n lÃ  tá»‘t)')
```

-----

## ğŸ’¡ Insight quan trá»ng tá»« dá»¯ liá»‡u (EDA)

Sau quÃ¡ trÃ¬nh phÃ¢n tÃ­ch khÃ¡m phÃ¡, chÃºng tÃ´i rÃºt ra cÃ¡c káº¿t luáº­n chÃ­nh áº£nh hÆ°á»Ÿng Ä‘áº¿n mÃ´ hÃ¬nh:

* **Má»©c Ä‘á»™ sá»­ dá»¥ng dá»‹ch vá»¥ y táº¿ (Healthcare Utilization):** Sá»‘ láº§n khÃ¡m bÃ¡c sÄ©, sá»‘ láº§n nháº­p viá»‡n vÃ  sá»‘ lÆ°á»£ng thuá»‘c sá»­ dá»¥ng lÃ  nhá»¯ng yáº¿u tá»‘ dá»± bÃ¡o hÃ ng Ä‘áº§u (Top 1).
* **Bá»‡nh mÃ£n tÃ­nh (Chronic Conditions):** Sá»‘ lÆ°á»£ng vÃ  loáº¡i bá»‡nh mÃ£n tÃ­nh tÃ¡c Ä‘á»™ng cá»±c lá»›n Ä‘áº¿n chi phÃ­.
* **TÆ°Æ¡ng tÃ¡c Tuá»•i Ã— Sá»©c khá»e:** NgÆ°á»i cao tuá»•i máº¯c nhiá»u bá»‡nh ná»n sáº½ cÃ³ chi phÃ­ tÄƒng theo **cáº¥p sá»‘ nhÃ¢n** chá»© khÃ´ng pháº£i phÃ©p cá»™ng Ä‘Æ¡n thuáº§n.
* **Äiá»ƒm rá»§i ro (Risk Scores):** CÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡ rá»§i ro Ä‘Æ°á»£c tÃ­nh toÃ¡n trÆ°á»›c cÃ³ giÃ¡ trá»‹ dá»± bÃ¡o ráº¥t cao.
* **Chá»‰ sá»‘ lÃ¢m sÃ ng:** Huyáº¿t Ã¡p, cholesterol vÃ  chá»‰ sá»‘ HbA1c Ä‘Ã³ng vai trÃ² quan trá»ng.
* **Yáº¿u tá»‘ báº£o hiá»ƒm:** Má»©c kháº¥u trá»« (deductibles), Ä‘á»“ng chi tráº£ (copays) vÃ  lá»‹ch sá»­ yÃªu cáº§u bá»“i thÆ°á»ng (claims history) Ä‘Ã³ng gÃ³p Ä‘Ã¡ng ká»ƒ vÃ o viá»‡c dá»± Ä‘oÃ¡n.

*![Yáº¿u tá»‘ áº£nh hÆ°á»Ÿng](regression_results/factor_affect.png)*

## ğŸ“Š Káº¿t quáº£ thá»±c nghiá»‡m & So sÃ¡nh (Model Evaluation)

Há»‡ thá»‘ng Ä‘Ã£ tá»± Ä‘á»™ng huáº¥n luyá»‡n vÃ  so sÃ¡nh nhiá»u thuáº­t toÃ¡n khÃ¡c nhau (Linear, Tree-based, Boosting). DÆ°á»›i Ä‘Ã¢y lÃ  káº¿t quáº£ Ä‘Ã¡nh giÃ¡ trÃªn táº­p kiá»ƒm thá»­ (Test Set):

### 1. Báº£ng xáº¿p háº¡ng hiá»‡u suáº¥t
*ÄÆ¡n vá»‹ Ä‘o lÆ°á»ng chÃ­nh: RMSE (Root Mean Squared Error) - CÃ ng tháº¥p cÃ ng tá»‘t.*

| Xáº¿p háº¡ng | MÃ´ hÃ¬nh (Model) | RMSE (Log Scale) | Nháº­n xÃ©t hiá»‡u nÄƒng |
| :---: | :--- | :---: | :--- |
| ğŸ† **1** | **Random Forest** | **0.1644** | **QuÃ¡n quÃ¢n.** Hiá»‡u suáº¥t vÆ°á»£t trá»™i nhá» kháº£ nÄƒng xá»­ lÃ½ phi tuyáº¿n tÃ­nh vÃ  tÆ°Æ¡ng tÃ¡c phá»©c táº¡p giá»¯a cÃ¡c biáº¿n. |
| ğŸ¥ˆ 2 | XGBoost | 0.1803 | Ã quÃ¢n. Tá»‘c Ä‘á»™ huáº¥n luyá»‡n ráº¥t nhanh vÃ  hiá»‡u suáº¥t gáº§n sÃ¡t vá»›i Random Forest. |
| ğŸ¥‰ 3 | LightGBM | 0.1819 | Tá»‘i Æ°u tÃ i nguyÃªn bá»™ nhá»›, ráº¥t phÃ¹ há»£p khi dataset má»Ÿ rá»™ng lá»›n hÆ¡n. |
| 4 | Decision Tree | 0.1964 | KhÃ¡ tá»‘t nhÆ°ng dá»… bá»‹ overfitting so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p Ensemble (Rá»«ng cÃ¢y). |
| 5 | Gradient Boosting | 0.1995 | á»”n Ä‘á»‹nh, nhÆ°ng trong trÆ°á»ng há»£p nÃ y chÆ°a tá»‘i Æ°u báº±ng XGBoost/LightGBM. |
| 6 | Linear Regression | 0.2182 | Hiá»‡u suáº¥t tháº¥p hÆ¡n nhÃ³m cÃ¢y. Chá»‰ báº¯t Ä‘Æ°á»£c cÃ¡c má»‘i quan há»‡ tuyáº¿n tÃ­nh cÆ¡ báº£n. |
| 7 | Ridge Regression | 0.2182 | TÆ°Æ¡ng tá»± Linear Regression, viá»‡c Ä‘iá»u chuáº©n (Regularization) khÃ´ng giÃºp cáº£i thiá»‡n nhiá»u trong trÆ°á»ng há»£p nÃ y. |
| 8 | AdaBoost | 0.2384 | Hiá»‡u suáº¥t kÃ©m áº¥n tÆ°á»£ng nháº¥t trong nhÃ³m Boosting á»Ÿ bÃ i toÃ¡n nÃ y. |
| 9 | Lasso Regression | 0.2884 | KÃ©m nháº¥t. Viá»‡c Ã©p cÃ¡c há»‡ sá»‘ vá» 0 (Feature Selection quÃ¡ máº¡nh) cÃ³ thá»ƒ Ä‘Ã£ lÃ m máº¥t thÃ´ng tin quan trá»ng. |

*(LÆ°u Ã½: RMSE Ä‘Æ°á»£c tÃ­nh trÃªn biáº¿n má»¥c tiÃªu `annual_medical_cost` Ä‘Ã£ qua xá»­ lÃ½ Log-transform)*

### 2. Biá»ƒu Ä‘á»“ so sÃ¡nh trá»±c quan
Biá»ƒu Ä‘á»“ dÆ°á»›i Ä‘Ã¢y minh há»a sá»± chÃªnh lá»‡ch vá» sai sá»‘ giá»¯a cÃ¡c mÃ´ hÃ¬nh, cho tháº¥y sá»± vÆ°á»£t trá»™i cá»§a nhÃ³m thuáº­t toÃ¡n **Ensemble Learning** (Random Forest, Boosting) so vá»›i cÃ¡c thuáº­t toÃ¡n truyá»n thá»‘ng.

*![Biá»ƒu Ä‘á»“ so sÃ¡nh RMSE cÃ¡c mÃ´ hÃ¬nh](regression_results/rmse_bar.png)*
*![Biá»ƒu Ä‘á»“ so sÃ¡nh MAE cÃ¡c mÃ´ hÃ¬nh](regression_results/mae_bar.png)*
*![Biá»ƒu Ä‘á»“ so sÃ¡nh MAPE cÃ¡c mÃ´ hÃ¬nh](regression_results/mape_bar.png)*
*![Biá»ƒu Ä‘á»“ so sÃ¡nh R^2 cÃ¡c mÃ´ hÃ¬nh](regression_results/r2_bar.png)*



### 3. PhÃ¢n tÃ­ch káº¿t quáº£
* **Chiáº¿n tháº¯ng cá»§a Tree-based Models:** Random Forest vÃ  XGBoost vÆ°á»£t trá»™i vÃ¬ dá»¯ liá»‡u y táº¿ chá»©a nhiá»u ngÆ°á»¡ng (thresholds) vÃ  tÆ°Æ¡ng tÃ¡c phi tuyáº¿n. VÃ­ dá»¥: BMI chá»‰ thá»±c sá»± lÃ m tÄƒng vá»t chi phÃ­ khi vÆ°á»£t qua má»©c 30 (bÃ©o phÃ¬) vÃ  Ä‘i kÃ¨m vá»›i viá»‡c hÃºt thuá»‘c. Linear Regression khÃ³ há»c Ä‘Æ°á»£c Ä‘iá»u nÃ y náº¿u khÃ´ng táº¡o biáº¿n tÆ°Æ¡ng tÃ¡c thá»§ cÃ´ng.
* **Äá»™ á»•n Ä‘á»‹nh:** Random Forest cho tháº¥y Ä‘á»™ biáº¿n thiÃªn tháº¥p (Low Variance) khi kiá»ƒm thá»­ chÃ©o (Cross-validation), chá»©ng tá» mÃ´ hÃ¬nh Ã­t bá»‹ Overfitting.



| Metric | GiÃ¡ trá»‹ (Log Scale) | Ã nghÄ©a |
| :--- | :--- | :--- |
| **RMSE** | \~0.1644 | Sai sá»‘ trung bÃ¬nh phÆ°Æ¡ng cÄƒn (Root Mean Squared Error) |
| **MAE** | \~0.1303 | Sai sá»‘ tuyá»‡t Ä‘á»‘i trung bÃ¬nh |
| **RÂ²** | \~0.9627 | Má»©c Ä‘á»™ giáº£i thÃ­ch Ä‘á»™ biáº¿n thiÃªn dá»¯ liá»‡u |
| **MAPE**| \~0.0173 | Sai sá»‘ pháº§n trÄƒm trung bÃ¬nh|

-----
*![Evaluattion](regression_results/best_model.png)*

## ğŸ“ Ghi chÃº cho Google Colab

Náº¿u cháº¡y trÃªn Google Colab, hÃ£y upload 3 file module (`preprocess.py`, `visualize.py`, `model_trainer.py`) vÃ o cÃ¹ng thÆ° má»¥c vá»›i Notebook, hoáº·c mount Google Drive:

```python
from google.colab import drive
drive.mount('/content/drive')
import sys
sys.path.append('/content/drive/MyDrive/path_to_your_project')
```

## ğŸš€ HÆ°á»›ng phÃ¡t triá»ƒn tiáº¿p theo (Roadmap)

DÃ¹ mÃ´ hÃ¬nh hiá»‡n táº¡i Ä‘Ã£ Ä‘áº¡t káº¿t quáº£ tá»‘t (RMSE ~0.16), dá»± Ã¡n váº«n cÃ³ thá»ƒ cáº£i thiá»‡n thÃªm:

* **Deploy Model:** XÃ¢y dá»±ng API báº±ng **FastAPI** hoáº·c **Flask** Ä‘á»ƒ phá»¥c vá»¥ dá»± Ä‘oÃ¡n realtime.
* **Dockerize:** ÄÃ³ng gÃ³i toÃ n bá»™ mÃ´i trÆ°á»ng cháº¡y vÃ o Docker Container Ä‘á»ƒ dá»… dÃ ng triá»ƒn khai.
* **Feature Selection nÃ¢ng cao:** Sá»­ dá»¥ng SHAP values Ä‘á»ƒ giáº£i thÃ­ch mÃ´ hÃ¬nh rÃµ rÃ ng hÆ¡n (Explainable AI).
* **Deep Learning:** Thá»­ nghiá»‡m máº¡ng nÆ¡-ron (Neural Network) vá»›i Keras/TensorFlow Ä‘á»ƒ xem cÃ³ vÆ°á»£t qua Ä‘Æ°á»£c Random Forest khÃ´ng.


## ğŸ‘¥ TÃ¡c giáº£

  * **Há» vÃ  tÃªn:** Nguyá»…n Há»¯u PhÆ°á»›c, Nguyá»…n ChÃ­ Tiáº¿n
 

-----

````



