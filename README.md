
Äá»“ Ã¡n cuá»‘i ká»³ mÃ´n Python cho khoa há»c dá»¯ liá»‡u

## ğŸ“‘ Má»¥c lá»¥c

1. [Bá»‘i cáº£nh & Má»¥c tiÃªu](#-bá»‘i-cáº£nh--má»¥c-tiÃªu)
2. [TÃ­nh nÄƒng ná»•i báº­t](#-tÃ­nh-nÄƒng-ná»•i-báº­t)
3. [Cáº¥u trÃºc dá»± Ã¡n](#-cáº¥u-trÃºc-dá»±-Ã¡n)
4. [CÃ i Ä‘áº·t (CÆ¡ báº£n)](#-cÃ i-Ä‘áº·t)
5. [HÆ°á»›ng dáº«n sá»­ dá»¥ng (Pipeline)](#-hÆ°á»›ng-dáº«n-sá»­-dá»¥ng)
6. [HÆ°á»›ng dáº«n cÃ i Ä‘áº·t & Cháº¡y trÃªn Local (Chi tiáº¿t)](#-hÆ°á»›ng-dáº«n-cÃ i-Ä‘áº·t--cháº¡y-trÃªn-local-mÃ¡y-cÃ¡-nhÃ¢n)
7. [Káº¿t quáº£ thá»±c nghiá»‡m & So sÃ¡nh](#-káº¿t-quáº£-thá»±c-nghiá»‡m--so-sÃ¡nh-model-evaluation)
8. [Ghi chÃº cho Google Colab](#-ghi-chÃº-cho-google-colab)
9. [HÆ°á»›ng phÃ¡t triá»ƒn tiáº¿p theo](#-hÆ°á»›ng-phÃ¡t-triá»ƒn-tiáº¿p-theo-roadmap)
10. [TÃ¡c giáº£](#-tÃ¡c-giáº£)



# ğŸ¥ Dá»± Ä‘oÃ¡n Chi PhÃ­ Y Táº¿ (Medical Cost Prediction)

Dá»± Ã¡n Machine Learning End-to-End nháº±m dá»± Ä‘oÃ¡n chi phÃ­ y táº¿ háº±ng nÄƒm (`annual_medical_cost`) dá»±a trÃªn há»“ sÆ¡ nhÃ¢n kháº©u há»c, sá»©c khá»e vÃ  báº£o hiá»ƒm cá»§a bá»‡nh nhÃ¢n. Dá»± Ã¡n Ä‘Æ°á»£c xÃ¢y dá»±ng theo hÆ°á»›ng Ä‘á»‘i tÆ°á»£ng (OOP) vá»›i cÃ¡c module tÃ¡i sá»­ dá»¥ng cao.

## ğŸ¯ Bá»‘i cáº£nh & Má»¥c tiÃªu
Chi phÃ­ chÄƒm sÃ³c sá»©c khá»e Ä‘ang lÃ  gÃ¡nh náº·ng lá»›n Ä‘á»‘i vá»›i nhiá»u cÃ¡ nhÃ¢n vÃ  tá»• chá»©c báº£o hiá»ƒm. Dá»± Ã¡n nÃ y Ä‘Æ°á»£c xÃ¢y dá»±ng nháº±m giáº£i quyáº¿t bÃ i toÃ¡n: **"Liá»‡u cÃ³ thá»ƒ dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c chi phÃ­ y táº¿ háº±ng nÄƒm dá»±a trÃªn há»“ sÆ¡ cÃ¡ nhÃ¢n?"**

Dá»¯ liá»‡u bao gá»“m 100.000 báº£n ghi vá»›i cÃ¡c nhÃ³m thÃ´ng tin:
* **Demographics:** Tuá»•i, giá»›i tÃ­nh, vÃ¹ng miá»n.
* **Lifestyle:** Chá»‰ sá»‘ BMI, hÃºt thuá»‘c, táº­p thá»ƒ dá»¥c.
* **Medical History:** Tiá»n sá»­ bá»‡nh lÃ½ (tiá»ƒu Ä‘Æ°á»ng, cao huyáº¿t Ã¡p...).
* **Insurance:** Loáº¡i gÃ³i báº£o hiá»ƒm, háº¡n má»©c.

**Má»¥c tiÃªu chÃ­nh:** XÃ¢y dá»±ng mÃ´ hÃ¬nh há»“i quy (Regression) Ä‘á»ƒ dá»± Ä‘oÃ¡n `annual_medical_cost`, giÃºp cÃ¡c cÃ´ng ty báº£o hiá»ƒm Ä‘Ã¡nh giÃ¡ rá»§i ro vÃ  cÃ¡ nhÃ¢n hÃ³a gÃ³i dá»‹ch vá»¥.

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![Library](https://img.shields.io/badge/Library-Scikit--Learn%20%7C%20Pandas%20%7C%20Seaborn-green)

## ğŸŒŸ TÃ­nh nÄƒng ná»•i báº­t

* **Pipeline Tiá»n xá»­ lÃ½ dá»¯ liá»‡u máº¡nh máº½ (`preprocess.py`):**
    * Tá»± Ä‘á»™ng xá»­ lÃ½ giÃ¡ trá»‹ thiáº¿u (Mean, Median, Forward-fill).
    * PhÃ¡t hiá»‡n vÃ  xá»­ lÃ½ ngoáº¡i lai (Outliers) báº±ng IQR hoáº·c Isolation Forest.
    * Feature Engineering: TÃ¡ch ngÃ y thÃ¡ng, One-Hot Encoding, Ordinal Encoding.
    * Chuáº©n hÃ³a dá»¯ liá»‡u (Scaling) Ä‘á»ƒ chá»‘ng rÃ² rá»‰ dá»¯ liá»‡u (Data Leakage).
* **Huáº¥n luyá»‡n mÃ´ hÃ¬nh tá»± Ä‘á»™ng (`model_trainer.py`):**
    * Há»— trá»£ cháº¡y Ä‘ua (Race) giá»¯a nhiá»u mÃ´ hÃ¬nh: Random Forest, XGBoost, Linear Regression, SVM...
    * Tá»± Ä‘á»™ng tá»‘i Æ°u tham sá»‘ (Hyperparameter Tuning) báº±ng Random Search.
    * LÆ°u trá»¯ artifact trá»n gÃ³i (Model + Scaler + Metrics).
* **Trá»±c quan hÃ³a dá»¯ liá»‡u (`visualize.py`):**
    * Há»‡ thá»‘ng váº½ biá»ƒu Ä‘á»“ chuáº©n hÃ³a, dá»… dÃ ng so sÃ¡nh hiá»‡u suáº¥t mÃ´ hÃ¬nh.
    * Há»— trá»£ váº½ Dashboard so sÃ¡nh Ä‘a chá»‰ sá»‘ (RMSE, MAE, R2).

## ğŸ“‚ Cáº¥u trÃºc dá»± Ã¡n

```text
â”œâ”€â”€ data/                       # Chá»©a file dá»¯ liá»‡u gá»‘c (csv, xlsx)
â”œâ”€â”€ results/                    # Káº¿t quáº£ Ä‘áº§u ra (Logs, Models, Charts)
â”‚   â”œâ”€â”€ best_model.pkl          # Model tá»‘t nháº¥t Ä‘Ã£ huáº¥n luyá»‡n
â”‚   â”œâ”€â”€ training.log            # Log quÃ¡ trÃ¬nh cháº¡y
â”‚   â””â”€â”€ model_comparison.csv    # Báº£ng so sÃ¡nh cÃ¡c model
â”‚   â””â”€â”€ cÃ¡c plot so sÃ¡nh cÃ¡c model
â”‚
â”œâ”€â”€ preprocessing/              # ThÆ° má»¥c module chÃ­nh
â”‚   â”œâ”€â”€ __init__.py             # File khá»Ÿi táº¡o module
â”‚   â”œâ”€â”€ base.py                 # Chá»©a BasePreprocessor
â”‚   â”œâ”€â”€ imputer.py              # Chá»©a class Imputer
â”‚   â”œâ”€â”€ scaler.py               # Chá»©a class Scaler
â”‚   â”œâ”€â”€ outlier_handler.py      # Chá»©a class OutlierHandler
â”‚   â”œâ”€â”€ feature_engineer.py     # Chá»©a class FeatureEngineer
â”‚   â””â”€â”€ manager.py              # Chá»©a class DataManager
â””â”€â”€ demo_preprocess.py          # File cháº¡y demo cho module preprocessing
â”‚
â”œâ”€â”€ model_training/             # Folder Module
â”‚   â”œâ”€â”€ __init__.py             # Khá»Ÿi táº¡o module
â”‚   â”œâ”€â”€ logger_config.py        # Cáº¥u hÃ¬nh logging
â”‚   â””â”€â”€ trainer.py              # Chá»©a class ModelTrainer chÃ­nh
â””â”€â”€ demo_training.py            # File script Ä‘á»ƒ cháº¡y demo module model_traning
â”‚
â”œâ”€â”€ visualize.py                # Module trá»±c quan hÃ³a
â”œâ”€â”€ EDA.ipynb                   # Notebook Ä‘á»ƒ cháº¡y pháº§n EDA cá»§a dá»± Ã¡n
â”œâ”€â”€ FE_MODELING.ipynb           # Notebook Ä‘á»ƒ cháº¡y feature engineering vÃ  modeling
â”œâ”€â”€ requirements.txt            # CÃ¡c thÆ° viá»‡n cáº§n thiáº¿t
â””â”€â”€ README.md                   # HÆ°á»›ng dáº«n sá»­ dá»¥ng
````

## ğŸ› ï¸ CÃ i Ä‘áº·t

### 1\. YÃªu cáº§u há»‡ thá»‘ng

  * Python 3.8 trá»Ÿ lÃªn.
  * CÃ¡c thÆ° viá»‡n: pandas, numpy, scikit-learn, matplotlib, seaborn, joblib, xgboost, lightgbm.

### 2\. CÃ i Ä‘áº·t thÆ° viá»‡n

Cháº¡y lá»‡nh sau Ä‘á»ƒ cÃ i Ä‘áº·t cÃ¡c gÃ³i cáº§n thiáº¿t:

```bash
pip install -r requirements.txt
```



-----

## ğŸš€ HÆ°á»›ng dáº«n sá»­ dá»¥ng

### BÆ°á»›c 1: Chuáº©n bá»‹ dá»¯ liá»‡u vÃ  Tiá»n xá»­ lÃ½

Sá»­ dá»¥ng `DataManager` vÃ  cÃ¡c bá»™ xá»­ lÃ½ trong module `preprocesing`.

```python
from preprocessing import DataManager, Imputer, Scaler, OutlierHandler, FeatureEngineer

# 1. Load dá»¯ liá»‡u
manager = DataManager('data/medical_cost.csv')

# 2. Äá»‹nh nghÄ©a Pipeline
steps = [
    Imputer(strategy='mean', columns=['bmi', 'income']),
    FeatureEngineer(one_hot_cols=['region', 'smoker'], ordinal_cols={'education': ['No HS', 'HS', 'Bachelor']}),
    OutlierHandler(method='isolation_forest', action='remove'),
    Scaler(method='standard')
]

# 3. Ãp dá»¥ng
for step in steps:
    manager.apply(step)

df_clean = manager.get_data()
```

### BÆ°á»›c 2: Huáº¥n luyá»‡n vÃ  So sÃ¡nh mÃ´ hÃ¬nh

Sá»­ dá»¥ng `model_training` Ä‘á»ƒ tá»± Ä‘á»™ng tÃ¬m mÃ´ hÃ¬nh tá»‘t nháº¥t.

```python
from model_training import ModelTrainer, setup_logging
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

# 1. Khá»Ÿi táº¡o
setup_logging()
trainer = ModelTrainer(task_type='regression')
trainer.load_data(df_clean.drop('annual_medical_cost', axis=1), df_clean['annual_medical_cost'])
trainer.split_data()

# 2. Cáº¥u hÃ¬nh cÃ¡c model cáº§n Ä‘ua
models_config = {
    'RandomForest': (RandomForestRegressor(), {'n_estimators': [50, 100]}),
    'GradientBoosting': (GradientBoostingRegressor(), {'learning_rate': [0.01, 0.1]})
}

# 3. Cháº¡y tá»± Ä‘á»™ng
trainer.auto_train(models_config, output_dir='results',scaler_type='standard')
```

### BÆ°á»›c 3: ÄÃ¡nh giÃ¡ vÃ  Trá»±c quan hÃ³a

Sá»­ dá»¥ng `DataVisualizer` Ä‘á»ƒ xem káº¿t quáº£.

```python
import pandas as pd
from visualize import DataVisualizer

# 1. Äá»c káº¿t quáº£ so sÃ¡nh
df_results = pd.read_csv('results/model_comparison.csv')

# 2. Váº½ biá»ƒu Ä‘á»“ so sÃ¡nh RMSE
viz = DataVisualizer(df_results)
viz.plot_bar(x='score', y='model', title='So sÃ¡nh RMSE (Tháº¥p hÆ¡n lÃ  tá»‘t)')
```

-----

-----


## ğŸ–¥ï¸ HÆ°á»›ng dáº«n cÃ i Ä‘áº·t & Cháº¡y trÃªn Local (MÃ¡y cÃ¡ nhÃ¢n)

Äá»ƒ Ä‘áº£m báº£o dá»± Ã¡n cháº¡y á»•n Ä‘á»‹nh vÃ  khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n cÃ¡c dá»± Ã¡n Python khÃ¡c trong mÃ¡y, chÃºng tÃ´i khuyáº¿n nghá»‹ sá»­ dá»¥ng **MÃ´i trÆ°á»ng áº£o (Virtual Environment)**.

### BÆ°á»›c 1: Clone dá»± Ã¡n vá» mÃ¡y
Má»Ÿ Terminal (hoáº·c CMD/PowerShell) vÃ  cháº¡y lá»‡nh:

```bash
# Clone repository (náº¿u báº¡n dÃ¹ng git)
git clone https://github.com/nguyenhuuphuc11052005/project_python.git
cd project_python

# Hoáº·c náº¿u báº¡n táº£i file zip, hÃ£y giáº£i nÃ©n vÃ  má»Ÿ terminal táº¡i thÆ° má»¥c Ä‘Ã³.
````

### BÆ°á»›c 2: Táº¡o mÃ´i trÆ°á»ng áº£o (Virtual Environment)

Viá»‡c nÃ y giÃºp cÃ´ láº­p cÃ¡c thÆ° viá»‡n cá»§a dá»± Ã¡n.

```bash
# Táº¡o mÃ´i trÆ°á»ng áº£o tÃªn lÃ  'venv'
python -m venv venv
```

### BÆ°á»›c 3: KÃ­ch hoáº¡t mÃ´i trÆ°á»ng áº£o

TÃ¹y thuá»™c vÃ o há»‡ Ä‘iá»u hÃ nh, lá»‡nh kÃ­ch hoáº¡t sáº½ khÃ¡c nhau:

  * **TrÃªn Windows:**
    ```bash
    .\venv\Scripts\activate
    ```
  * **TrÃªn macOS / Linux:**
    ```bash
    source venv/bin/activate
    ```

*(Sau khi kÃ­ch hoáº¡t, báº¡n sáº½ tháº¥y chá»¯ `(venv)` xuáº¥t hiá»‡n á»Ÿ Ä‘áº§u dÃ²ng lá»‡nh)*

### BÆ°á»›c 4: CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n phá»¥ thuá»™c

Cháº¡y lá»‡nh sau Ä‘á»ƒ cÃ i Ä‘áº·t toÃ n bá»™ thÆ° viá»‡n cáº§n thiáº¿t tá»« file `requirements.txt`:

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### BÆ°á»›c 5: Cáº¥u trÃºc thÆ° má»¥c dá»¯ liá»‡u

Äáº£m báº£o báº¡n Ä‘Ã£ táº£i file dá»¯ liá»‡u vÃ  Ä‘áº·t Ä‘Ãºng vá»‹ trÃ­ (vÃ¬ code máº·c Ä‘á»‹nh Ä‘á»c tá»« thÆ° má»¥c `data/` hoáº·c thÆ° má»¥c gá»‘c tÃ¹y cáº¥u hÃ¬nh):

```text
project_python/
â”‚â”€â”€ medical_insurance.csv  <-- File dá»¯ liá»‡u cá»§a báº¡n Ä‘áº·t á»Ÿ Ä‘Ã¢y
â”‚â”€â”€ preprocessing
â”‚â”€â”€ model_training
â”‚â”€â”€ ...
```

### BÆ°á»›c 6: Cháº¡y dá»± Ã¡n

Báº¡n cÃ³ 2 cÃ¡ch Ä‘á»ƒ cháº¡y:

**CÃ¡ch 1: Cháº¡y tá»«ng module (KhuyÃªn dÃ¹ng Ä‘á»ƒ test)**
Má»—i module Ä‘á»u cÃ³ sáºµn pháº§n `if __name__ == "__main__":` Ä‘á»ƒ cháº¡y demo.

```bash
# 1. Cháº¡y thá»­ quy trÃ¬nh xá»­ lÃ½ dá»¯ liá»‡u
python demo_preprocess.py

# 2. Cháº¡y thá»­ quy trÃ¬nh huáº¥n luyá»‡n vÃ  so sÃ¡nh model
python demo_training.py

# 3. Cháº¡y thá»­ váº½ biá»ƒu Ä‘á»“ demo
python visualize.py
```

**CÃ¡ch 2: Cháº¡y trÃªn Jupyter Notebook**
Náº¿u báº¡n muá»‘n cháº¡y file `EDA.ipynb`, `FE_MODELING.ipynb` Ä‘á»ƒ phÃ¢n tÃ­ch tá»«ng bÆ°á»›c:

```bash
# CÃ i Ä‘áº·t jupyter náº¿u chÆ°a cÃ³
pip install jupyterlab

# Khá»Ÿi Ä‘á»™ng notebook
jupyter lab
```

Sau Ä‘Ã³ má»Ÿ file `EDA.ipynb` chá»‰nh sá»­a láº¡i dÃ²ng 
```bash
%cd path_to_your_project
```

cháº¡y (Run All) Ä‘á»ƒ xem EDA vÃ  xá»­ lÃ½ missing data. Rá»“i sau Ä‘Ã³ má»›i má»Ÿ file `FE_MODELING.ipynb` chá»‰nh sá»­a láº¡i dÃ²ng
```bash
%cd path_to_your_project
```
vÃ  cháº¡y (Run All).

------



## ğŸ“Š Káº¿t quáº£ thá»±c nghiá»‡m & So sÃ¡nh (Model Evaluation)

Há»‡ thá»‘ng Ä‘Ã£ tá»± Ä‘á»™ng huáº¥n luyá»‡n vÃ  so sÃ¡nh nhiá»u thuáº­t toÃ¡n khÃ¡c nhau (Linear, Tree-based, Boosting). DÆ°á»›i Ä‘Ã¢y lÃ  káº¿t quáº£ Ä‘Ã¡nh giÃ¡ trÃªn táº­p kiá»ƒm thá»­ (Test Set):

### 1. Báº£ng xáº¿p háº¡ng hiá»‡u suáº¥t
*ÄÆ¡n vá»‹ Ä‘o lÆ°á»ng chÃ­nh: RMSE (Root Mean Squared Error) - CÃ ng tháº¥p cÃ ng tá»‘t.*


| Xáº¿p háº¡ng | MÃ´ hÃ¬nh (Model) | RMSE | RÂ² Score | Nháº­n xÃ©t chi tiáº¿t |
| :---: | :--- | :---: | :---: | :--- |
| ğŸ† **1** | **XGBoost** | **0.1624** | **0.9636** | **QuÃ¡n quÃ¢n.** Äáº¡t Ä‘á»™ lá»—i tháº¥p nháº¥t. Kháº£ nÄƒng tá»‘i Æ°u hÃ³a gradient boosting cá»±c tá»‘t giÃºp mÃ´ hÃ¬nh náº¯m báº¯t chÃ­nh xÃ¡c cÃ¡c máº«u dá»¯ liá»‡u phá»©c táº¡p. |
| ğŸ¥ˆ 2 | LightGBM | 0.1625 | 0.9635 | **Ã quÃ¢n.** Hiá»‡u nÄƒng gáº§n nhÆ° ngang ngá»­a XGBoost (chÃªnh lá»‡ch khÃ´ng Ä‘Ã¡ng ká»ƒ), nhÆ°ng thÆ°á»ng cÃ³ lá»£i tháº¿ vá» tá»‘c Ä‘á»™ huáº¥n luyá»‡n nhanh hÆ¡n. |
| ğŸ¥‰ 3 | Random Forest | 0.1638 | 0.9630 | Ráº¥t á»•n Ä‘á»‹nh. Tuy nhiÃªn á»Ÿ dataset nÃ y, phÆ°Æ¡ng phÃ¡p Boosting (XGBoost/LightGBM) Ä‘Ã£ chá»©ng minh hiá»‡u quáº£ hÆ¡n phÆ°Æ¡ng phÃ¡p Bagging. |
| 4 | Gradient Boosting | 0.1651 | 0.9624 | Hiá»‡u quáº£ cao, xáº¿p ngay sau top 3. LÃ  ná»n táº£ng tá»‘t nhÆ°ng chÆ°a tá»‘i Æ°u báº±ng cÃ¡c phiÃªn báº£n cáº£i tiáº¿n nhÆ° XGB/LGBM. |
| 5 | Decision Tree | 0.1706 | 0.9598 | KhÃ¡ áº¥n tÆ°á»£ng Ä‘á»‘i vá»›i má»™t mÃ´ hÃ¬nh Ä‘Æ¡n láº», nhÆ°ng váº«n thua kÃ©m cÃ¡c mÃ´ hÃ¬nh tá»• há»£p (Ensemble) do kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a kÃ©m hÆ¡n. |
| 6 | AdaBoost | 0.2050 | 0.9420 | Hiá»‡u suáº¥t trung bÃ¬nh khÃ¡. CÆ¡ cháº¿ trá»ng sá»‘ thÃ­ch nghi chÆ°a phÃ¡t huy tÃ¡c dá»¥ng tá»‘i Ä‘a so vá»›i Gradient Boosting á»Ÿ bÃ i toÃ¡n nÃ y. |
| 7 | Ridge Regression | 0.2180 | 0.9344 | Tá»‘t hÆ¡n Linear Regression má»™t chÃºt xÃ­u nhá» Regularization, nhÆ°ng váº«n khÃ´ng báº¯t Ä‘Æ°á»£c cÃ¡c má»‘i quan há»‡ phi tuyáº¿n tÃ­nh. |
| 8 | Linear Regression | 0.2180 | 0.9344 | MÃ´ hÃ¬nh cÆ¡ sá»Ÿ (Baseline). Hiá»‡u suáº¥t tháº¥p hÆ¡n nhÃ³m cÃ¢y quyáº¿t Ä‘á»‹nh, cho tháº¥y dá»¯ liá»‡u cÃ³ tÃ­nh phi tuyáº¿n cao. |
| 9 | Lasso Regression | 0.2884 | 0.8852 | **KÃ©m nháº¥t.** Viá»‡c triá»‡t tiÃªu cÃ¡c biáº¿n (Feature Selection máº¡nh tay) dÆ°á»ng nhÆ° Ä‘Ã£ lÃ m máº¥t Ä‘i nhiá»u thÃ´ng tin quan trá»ng, dáº«n Ä‘áº¿n underfitting. |

*(LÆ°u Ã½: RMSE Ä‘Æ°á»£c tÃ­nh trÃªn biáº¿n má»¥c tiÃªu `annual_medical_cost` Ä‘Ã£ qua xá»­ lÃ½ Log-transform)*



### 3. PhÃ¢n tÃ­ch káº¿t quáº£
* **Chiáº¿n tháº¯ng cá»§a Tree-based Models:** Random Forest , LightGBM, XGBoost vÆ°á»£t trá»™i vÃ¬ dá»¯ liá»‡u y táº¿ chá»©a nhiá»u ngÆ°á»¡ng (thresholds) vÃ  tÆ°Æ¡ng tÃ¡c phi tuyáº¿n. VÃ­ dá»¥: BMI chá»‰ thá»±c sá»± lÃ m tÄƒng vá»t chi phÃ­ khi vÆ°á»£t qua má»©c 30 (bÃ©o phÃ¬) vÃ  Ä‘i kÃ¨m vá»›i viá»‡c hÃºt thuá»‘c. Linear Regression khÃ³ há»c Ä‘Æ°á»£c Ä‘iá»u nÃ y náº¿u khÃ´ng táº¡o biáº¿n tÆ°Æ¡ng tÃ¡c thá»§ cÃ´ng.
* **Äá»™ á»•n Ä‘á»‹nh:** Random Forest cho tháº¥y Ä‘á»™ biáº¿n thiÃªn tháº¥p (Low Variance) khi kiá»ƒm thá»­ chÃ©o (Cross-validation), chá»©ng tá» mÃ´ hÃ¬nh Ã­t bá»‹ Overfitting.



| Metric | GiÃ¡ trá»‹ (Log Scale) | Ã nghÄ©a |
| :--- | :--- | :--- |
| **RMSE** | \~0.1624 | Sai sá»‘ trung bÃ¬nh phÆ°Æ¡ng cÄƒn (Root Mean Squared Error) |
| **MAE** | \~0.1291 | Sai sá»‘ tuyá»‡t Ä‘á»‘i trung bÃ¬nh |
| **RÂ²** | \~0.9636 | Má»©c Ä‘á»™ giáº£i thÃ­ch Ä‘á»™ biáº¿n thiÃªn dá»¯ liá»‡u |
| **MAPE**| \~0.0171 | Sai sá»‘ pháº§n trÄƒm trung bÃ¬nh|

-----


## ğŸ“ Ghi chÃº cho Google Colab

Náº¿u cháº¡y trÃªn Google Colab, hÃ£y upload 3 file module (`preprocess.py`, `visualize.py`, `model_trainer.py`) vÃ o cÃ¹ng thÆ° má»¥c vá»›i Notebook, hoáº·c mount Google Drive:

```python
from google.colab import drive
drive.mount('/content/drive')
import sys
%cd /content/drive/MyDrive/path_to_your_project
```

## ğŸš€ HÆ°á»›ng phÃ¡t triá»ƒn tiáº¿p theo (Roadmap)

DÃ¹ mÃ´ hÃ¬nh hiá»‡n táº¡i Ä‘Ã£ Ä‘áº¡t káº¿t quáº£ tá»‘t (RMSE ~0.16), dá»± Ã¡n váº«n cÃ³ thá»ƒ cáº£i thiá»‡n thÃªm:

* **Deploy Model:** XÃ¢y dá»±ng API báº±ng **FastAPI** hoáº·c **Flask** Ä‘á»ƒ phá»¥c vá»¥ dá»± Ä‘oÃ¡n realtime.
* **Dockerize:** ÄÃ³ng gÃ³i toÃ n bá»™ mÃ´i trÆ°á»ng cháº¡y vÃ o Docker Container Ä‘á»ƒ dá»… dÃ ng triá»ƒn khai.
* **Feature Selection nÃ¢ng cao:** Sá»­ dá»¥ng SHAP values Ä‘á»ƒ giáº£i thÃ­ch mÃ´ hÃ¬nh rÃµ rÃ ng hÆ¡n (Explainable AI).
* **Deep Learning:** Thá»­ nghiá»‡m máº¡ng nÆ¡-ron (Neural Network) vá»›i Keras/TensorFlow Ä‘á»ƒ xem cÃ³ vÆ°á»£t qua Ä‘Æ°á»£c Random Forest khÃ´ng.


## ğŸ‘¥ TÃ¡c giáº£

  * **Há» vÃ  tÃªn:** Nguyá»…n Há»¯u PhÆ°á»›c, MSSV: 23280078
  * **Há» vÃ  tÃªn:** Nguyá»…n ChÃ­ Tiáº¿n, MSSV: 23280087
 

-----